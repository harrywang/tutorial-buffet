{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://microsoft.github.io/autogen/docs/tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "print(random.randint(0, 9999999999999999))\n",
    "llm_config={\"config_list\": [{\n",
    "    \"model\": \"gpt-4-turbo\",\n",
    "    \"cache\": None,\n",
    "    \"temperature\": 0.9, \n",
    "    \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]}\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke\", \"role\": \"user\"}])\n",
    "print(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run multiple times\n",
    "for i in range(3):\n",
    "    print(random.randint(0, 9999999999999999))\n",
    "    llm_config={\"config_list\": [{\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"cache_seed\": random.randint(0, 9999999999999999),\n",
    "        \"temperature\": 0.9, \n",
    "        \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]}\n",
    "\n",
    "    agent = ConversableAgent(\n",
    "        \"chatbot\",\n",
    "        llm_config=llm_config,\n",
    "        code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "        function_map=None,  # No registered functions, by default it is None.\n",
    "        human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    )\n",
    "\n",
    "    reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke\", \"role\": \"user\"}])\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a movie\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cathy = ConversableAgent(\n",
    "    \"cathy\",\n",
    "    system_message=\"Your name is Cathy and you are a part of a duo of comedians.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config=llm_config,\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\", max_turns=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
    ")\n",
    "\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joe = ConversableAgent(\n",
    "    \"joe\",\n",
    "    system_message=\"Your name is Joe and you are a part of a duo of comedians.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    is_termination_msg=lambda msg: \"good bye\" in msg[\"content\"].lower(),\n",
    ")\n",
    "\n",
    "result = joe.initiate_chat(cathy, message=\"Cathy, tell me a joke and then say the words GOOD BYE.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent_with_number = ConversableAgent(\n",
    "    \"agent_with_number\",\n",
    "    system_message=\"You are playing a game of guess-my-number. You have the \"\n",
    "    \"number 53 in your mind, and I will try to guess it. \"\n",
    "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"NEVER\",  # never ask for human input\n",
    ")\n",
    "\n",
    "agent_guess_number = ConversableAgent(\n",
    "    \"agent_guess_number\",\n",
    "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
    "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
    "    \"you should guess a higher number. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "result = agent_with_number.initiate_chat(\n",
    "    agent_guess_number,\n",
    "    message=\"I have a number between 1 and 100. Guess it!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_proxy = ConversableAgent(\n",
    "    \"human_proxy\",\n",
    "    llm_config=False,  # no LLM used for human proxy\n",
    "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
    ")\n",
    "\n",
    "# Start a chat with the agent with number with an initial guess.\n",
    "result = human_proxy.initiate_chat(\n",
    "    agent_with_number,  # this is the same agent with the number as before\n",
    "    message=\"10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_number = ConversableAgent(\n",
    "    \"agent_with_number\",\n",
    "    system_message=\"You are playing a game of guess-my-number. \"\n",
    "    \"In the first game, you have the \"\n",
    "    \"number 53 in your mind, and I will try to guess it. \"\n",
    "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    max_consecutive_auto_reply=1,  # maximum number of consecutive auto-replies before asking for human input\n",
    "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"TERMINATE\",  # ask for human input until the game is terminated\n",
    ")\n",
    "\n",
    "agent_guess_number = ConversableAgent(\n",
    "    \"agent_guess_number\",\n",
    "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
    "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
    "    \"you should guess a higher number. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "result = agent_with_number.initiate_chat(\n",
    "    agent_guess_number,\n",
    "    message=\"I have a number between 1 and 100. Guess it!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "# Create a temporary directory to store the code files.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Create a local command line code executor.\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=10,  # Timeout for each code execution in seconds.\n",
    "    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.\n",
    ")\n",
    "\n",
    "# Create an agent with code executor configuration.\n",
    "code_executor_agent = ConversableAgent(\n",
    "    \"code_executor_agent\",\n",
    "    llm_config=False,  # Turn off LLM for this agent.\n",
    "    code_execution_config={\"executor\": executor},  # Use the local command line code executor.\n",
    "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_with_code_block = \"\"\"This is a message with code block.\n",
    "The code block is below:\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.random.randint(0, 100, 100)\n",
    "y = np.random.randint(0, 100, 100)\n",
    "plt.scatter(x, y)\n",
    "plt.savefig('scatter.png')\n",
    "print('Scatter plot saved to scatter.png')\n",
    "```\n",
    "This is the end of the message.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a reply for the given code.\n",
    "reply = code_executor_agent.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(os.listdir(temp_dir.name))\n",
    "# open the scatter plot image\n",
    "img = plt.imread(os.path.join(temp_dir.name, \"scatter.png\"))\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.coding import DockerCommandLineCodeExecutor\n",
    "\n",
    "# Create a temporary directory to store the code files.\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Create a Docker command line code executor.\n",
    "executor = DockerCommandLineCodeExecutor(\n",
    "    image=\"python:3.12-slim\",  # Execute code using the given docker image name.\n",
    "    timeout=10,  # Timeout for each code execution in seconds.\n",
    "    work_dir=temp_dir.name,  # Use the temporary directory to store the code files.\n",
    ")\n",
    "\n",
    "# Create an agent with code executor configuration that uses docker.\n",
    "code_executor_agent_using_docker = ConversableAgent(\n",
    "    \"code_executor_agent_docker\",\n",
    "    llm_config=False,  # Turn off LLM for this agent.\n",
    "    code_execution_config={\"executor\": executor},  # Use the docker command line code executor.\n",
    "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
    ")\n",
    "\n",
    "\n",
    "#  a docker image is started\n",
    "# When the code executor is no longer used, stop it to release the resources.\n",
    "# executor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_with_code_block = \"\"\"This is a message with code block.\n",
    "The code block is below:\n",
    "```python\n",
    "print('Scatter plot saved to scatter.png')\n",
    "```\n",
    "This is the end of the message.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a reply for the given code.\n",
    "reply = code_executor_agent_using_docker.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_with_code_block = \"\"\"This is a message with code block.\n",
    "The code block is below:\n",
    "```shell\n",
    "pip install matplotlib numpy\n",
    "```\n",
    "This is the end of the message.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a reply for the given code.\n",
    "reply = code_executor_agent_using_docker.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_with_code_block = \"\"\"This is a message with code block.\n",
    "The code block is below:\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.random.randint(0, 100, 100)\n",
    "y = np.random.randint(0, 100, 100)\n",
    "plt.scatter(x, y)\n",
    "plt.savefig('scatter.png')\n",
    "print('Scatter plot saved to scatter.png')\n",
    "```\n",
    "This is the end of the message.\n",
    "\"\"\"\n",
    "\n",
    "# Generate a reply for the given code.\n",
    "reply = code_executor_agent_using_docker.generate_reply(messages=[{\"role\": \"user\", \"content\": message_with_code_block}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code writer agent's system message is to instruct the LLM on how to use\n",
    "# the code executor in the code executor agent.\n",
    "code_writer_system_message = \"\"\"You are a helpful AI assistant.\n",
    "Solve tasks using your coding and language skills.\n",
    "In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\n",
    "1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\n",
    "2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\n",
    "Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\n",
    "When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\n",
    "If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\n",
    "If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\n",
    "When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\n",
    "Reply 'TERMINATE' in the end when everything is done.\n",
    "\"\"\"\n",
    "\n",
    "code_writer_agent = ConversableAgent(\n",
    "    \"code_writer_agent\",\n",
    "    system_message=code_writer_system_message,\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    code_execution_config=False,  # Turn off code execution for this agent.\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "\n",
    "# Create a temporary directory to store the code files.\n",
    "temp_dir = \"./tmp\"\n",
    "\n",
    "# Create a local command line code executor.\n",
    "executor = LocalCommandLineCodeExecutor(\n",
    "    timeout=10,  # Timeout for each code execution in seconds.\n",
    "    work_dir=temp_dir,  # Use the temporary directory to store the code files.\n",
    ")\n",
    "\n",
    "# Create an agent with code executor configuration.\n",
    "code_executor_agent = ConversableAgent(\n",
    "    \"code_executor_agent\",\n",
    "    llm_config=False,  # Turn off LLM for this agent.\n",
    "    code_execution_config={\"executor\": executor},  # Use the local command line code executor.\n",
    "    human_input_mode=\"ALWAYS\",  # Always take human input for this agent for safety.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = code_executor_agent.initiate_chat(\n",
    "    code_writer_agent,\n",
    "    message=\"Write Python code to calculate the 14th Fibonacci number.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_result = code_executor_agent.initiate_chat(\n",
    "    code_writer_agent,\n",
    "    message=f\"Today is {today}. Write Python code to plot TSLA's and META's \"\n",
    "    \"stock price gains YTD, and save the plot to a file named 'stock_gains.png'.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "Operator = Literal[\"+\", \"-\", \"*\", \"/\"]\n",
    "\n",
    "\n",
    "def calculator(a: int, b: int, operator: Annotated[Operator, \"operator\"]) -> int:\n",
    "    if operator == \"+\":\n",
    "        return a + b\n",
    "    elif operator == \"-\":\n",
    "        return a - b\n",
    "    elif operator == \"*\":\n",
    "        return a * b\n",
    "    elif operator == \"/\":\n",
    "        return int(a / b)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "# Let's first define the assistant agent that suggests tool calls.\n",
    "assistant = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can help with simple calculations. \"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "# The user proxy agent is used for interacting with the assistant agent\n",
    "# and executes tool calls.\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# Register the tool signature with the assistant agent.\n",
    "assistant.register_for_llm(name=\"calculator\", description=\"A simple calculator\")(calculator)\n",
    "\n",
    "# Register the tool function with the user proxy agent.\n",
    "user_proxy.register_for_execution(name=\"calculator\")(calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import register_function\n",
    "\n",
    "# Register the calculator function to the two agents.\n",
    "register_function(\n",
    "    calculator,\n",
    "    caller=assistant,  # The assistant agent can suggest calls to the calculator.\n",
    "    executor=user_proxy,  # The user proxy agent can execute the calculator calls.\n",
    "    name=\"calculator\",  # By default, the function name is used as the tool name.\n",
    "    description=\"A simple calculator\",  # A description of the tool.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = user_proxy.initiate_chat(assistant, message=\"What is (44232 + 13312 / (232 - 32)) * 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(44232 + int(13312 / (232 - 32))) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Schema\n",
    "assistant.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class CalculatorInput(BaseModel):\n",
    "    a: Annotated[int, Field(description=\"The first number.\")]\n",
    "    b: Annotated[int, Field(description=\"The second number.\")]\n",
    "    operator: Annotated[Operator, Field(description=\"The operator.\")]\n",
    "\n",
    "\n",
    "def calculator(input: Annotated[CalculatorInput, \"Input to the calculator.\"]) -> int:\n",
    "    if input.operator == \"+\":\n",
    "        return input.a + input.b\n",
    "    elif input.operator == \"-\":\n",
    "        return input.a - input.b\n",
    "    elif input.operator == \"*\":\n",
    "        return input.a * input.b\n",
    "    elif input.operator == \"/\":\n",
    "        return int(input.a / input.b)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.register_for_llm(name=\"calculator\", description=\"A calculator tool that accepts nested expression as input\")(\n",
    "    calculator\n",
    ")\n",
    "user_proxy.register_for_execution(name=\"calculator\")(calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = user_proxy.initiate_chat(assistant, message=\"What is (1423 - 123) / 3 + (32 + 23) * 5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int((1423 - 123) / 3) + (32 + 23) * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "student_agent = ConversableAgent(\n",
    "    name=\"Student_Agent\",\n",
    "    system_message=\"You are a student willing to learn.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "teacher_agent = ConversableAgent(\n",
    "    name=\"Teacher_Agent\",\n",
    "    system_message=\"You are a math teacher.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "chat_result = student_agent.initiate_chat(\n",
    "    teacher_agent,\n",
    "    message=\"What is triangle inequality?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    "    max_turns=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ConversableAgent.DEFAULT_SUMMARY_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the chat history.\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(chat_result.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Number Agent always returns the same numbers.\n",
    "number_agent = ConversableAgent(\n",
    "    name=\"Number_Agent\",\n",
    "    system_message=\"You return me the numbers I give you, one number each line.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Adder Agent adds 1 to each number it receives.\n",
    "adder_agent = ConversableAgent(\n",
    "    name=\"Adder_Agent\",\n",
    "    system_message=\"You add 1 to each number I give you and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Multiplier Agent multiplies each number it receives by 2.\n",
    "multiplier_agent = ConversableAgent(\n",
    "    name=\"Multiplier_Agent\",\n",
    "    system_message=\"You multiply each number I give you by 2 and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Subtracter Agent subtracts 1 from each number it receives.\n",
    "subtracter_agent = ConversableAgent(\n",
    "    name=\"Subtracter_Agent\",\n",
    "    system_message=\"You subtract 1 from each number I give you and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "# The Divider Agent divides each number it receives by 2.\n",
    "divider_agent = ConversableAgent(\n",
    "    name=\"Divider_Agent\",\n",
    "    system_message=\"You divide each number I give you by 2 and return me the new numbers, one number each line.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a sequence of two-agent chats.\n",
    "# Each element in the list is a dictionary that specifies the arguments\n",
    "# for the initiate_chat method.\n",
    "chat_results = number_agent.initiate_chats(\n",
    "    [\n",
    "        {\n",
    "            \"recipient\": adder_agent,\n",
    "            \"message\": \"14\",\n",
    "            \"max_turns\": 2,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": multiplier_agent,\n",
    "            \"message\": \"These are my numbers\",\n",
    "            \"max_turns\": 2,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": subtracter_agent,\n",
    "            \"message\": \"These are my numbers\",\n",
    "            \"max_turns\": 2,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": divider_agent,\n",
    "            \"message\": \"These are my numbers\",\n",
    "            \"max_turns\": 2,\n",
    "            \"summary_method\": \"last_msg\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First Chat Summary: \", chat_results[0].summary)\n",
    "print(\"Second Chat Summary: \", chat_results[1].summary)\n",
    "print(\"Third Chat Summary: \", chat_results[2].summary)\n",
    "print(\"Fourth Chat Summary: \", chat_results[3].summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `description` attribute is a string that describes the agent.\n",
    "# It can also be set in `ConversableAgent` constructor.\n",
    "adder_agent.description = \"Add 1 to each input number.\"\n",
    "multiplier_agent.description = \"Multiply each input number by 2.\"\n",
    "subtracter_agent.description = \"Subtract 1 from each input number.\"\n",
    "divider_agent.description = \"Divide each input number by 2.\"\n",
    "number_agent.description = \"Return the numbers given.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChatManager\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result = number_agent.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"My number is 3, I want to turn it into 13.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_result.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat_with_introductions = GroupChat(\n",
    "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    "    send_introductions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the group chat with introduction messages created above.\n",
    "group_chat_manager_with_intros = GroupChatManager(\n",
    "    groupchat=group_chat_with_introductions,\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "# Start a sequence of two-agent chats between the number agent and\n",
    "# the group chat manager.\n",
    "chat_result = number_agent.initiate_chats(\n",
    "    [\n",
    "        {\n",
    "            \"recipient\": group_chat_manager_with_intros,\n",
    "            \"message\": \"My number is 3, I want to turn it into 13.\",\n",
    "        },\n",
    "        {\n",
    "            \"recipient\": group_chat_manager_with_intros,\n",
    "            \"message\": \"Turn this number to 32.\",\n",
    "        },\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_transitions = {\n",
    "    number_agent: [adder_agent, number_agent],\n",
    "    adder_agent: [multiplier_agent, number_agent],\n",
    "    subtracter_agent: [divider_agent, number_agent],\n",
    "    multiplier_agent: [subtracter_agent, number_agent],\n",
    "    divider_agent: [adder_agent, number_agent],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrained_graph_chat = GroupChat(\n",
    "    agents=[adder_agent, multiplier_agent, subtracter_agent, divider_agent, number_agent],\n",
    "    allowed_or_disallowed_speaker_transitions=allowed_transitions,\n",
    "    speaker_transitions_type=\"allowed\",\n",
    "    messages=[],\n",
    "    max_round=12,\n",
    "    send_introductions=True,\n",
    ")\n",
    "\n",
    "constrained_group_chat_manager = GroupChatManager(\n",
    "    groupchat=constrained_graph_chat,\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    ")\n",
    "\n",
    "chat_result = number_agent.initiate_chat(\n",
    "    constrained_group_chat_manager,\n",
    "    message=\"My number is 3, I want to turn it into 10. Once I get to 10, keep it there.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "\n",
    "arithmetic_agent = ConversableAgent(\n",
    "    name=\"Arithmetic_Agent\",\n",
    "    llm_config=False,\n",
    "    human_input_mode=\"ALWAYS\",\n",
    "    # This agent will always require human input to make sure the code is\n",
    "    # safe to execute.\n",
    "    code_execution_config={\"use_docker\": False, \"work_dir\": temp_dir},\n",
    ")\n",
    "\n",
    "code_writer_agent = ConversableAgent(\n",
    "    name=\"Code_Writer_Agent\",\n",
    "    system_message=\"You are a code writer. You write Python script in Markdown code blocks.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "poetry_agent = ConversableAgent(\n",
    "    name=\"Poetry_Agent\",\n",
    "    system_message=\"You are an AI poet.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_chats = [\n",
    "    {\n",
    "        \"recipient\": group_chat_manager_with_intros,\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "        \"summary_prompt\": \"Summarize the sequence of operations used to turn \" \"the source number into target number.\",\n",
    "    },\n",
    "    {\n",
    "        \"recipient\": code_writer_agent,\n",
    "        \"message\": \"Write a Python script to verify the arithmetic operations is correct.\",\n",
    "        \"summary_method\": \"reflection_with_llm\",\n",
    "    },\n",
    "    {\n",
    "        \"recipient\": poetry_agent,\n",
    "        \"message\": \"Write a poem about it.\",\n",
    "        \"max_turns\": 1,\n",
    "        \"summary_method\": \"last_msg\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arithmetic_agent.register_nested_chats(\n",
    "    nested_chats,\n",
    "    # The trigger function is used to determine if the agent should start the nested chat\n",
    "    # given the sender agent.\n",
    "    # In this case, the arithmetic agent will not start the nested chats if the sender is\n",
    "    # from the nested chats' recipient to avoid recursive calls.\n",
    "    trigger=lambda sender: sender not in [group_chat_manager_with_intros, code_writer_agent, poetry_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of using `initiate_chat` method to start another conversation,\n",
    "# we can use the `generate_reply` method to get single reply to a message directly.\n",
    "reply = arithmetic_agent.generate_reply(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I have a number 3 and I want to turn it into 7.\"}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
